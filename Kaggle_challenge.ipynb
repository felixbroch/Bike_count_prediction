{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import ydata_profiling\n",
    "from skrub import TableReport\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from vacances_scolaires_france import SchoolHolidayDates\n",
    "from datetime import date\n",
    "from jours_feries_france import JoursFeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(Path(\"data\") / \"train.parquet\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_parquet(Path(\"data\") / \"final_test.parquet\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_conditions = pd.read_csv('data/external_data.csv')\n",
    "external_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sort the `external_conditions` DataFrame by the `date` column\n",
    "external_conditions = external_conditions.sort_values(by='date')\n",
    "\n",
    "# Step 2: Remove duplicate entries based on the `date` column\n",
    "external_conditions = external_conditions.drop_duplicates(subset='date')\n",
    "\n",
    "filled_external_conditions = utils._fill_dataframe(external_conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_external_conditions = utils._column_rename(filled_external_conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data, test_merged_data = utils._merge_data_with_external_data(filled_external_conditions, data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_with_dates = utils._process_datetime_features(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TableReport(merged_data_with_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide to remove site id, site name and counter id to just keep counter name to reduce complexity and the data as they all provide more or less the same information. Counter is more precise as we will be able to calculate the number of times a counter is used in a given site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training with Elastic Net (To find the best features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic net can handle multicolinearity and shrinks the less important features to zero. It is a combination of L1 and L2 regularization. It is a linear regression model trained with L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and target variable\n",
    "X = merged_data.drop(columns=[\n",
    "                            'bike_count', 'log_bike_count',\n",
    "                            'counter_id', 'site_id', 'site_name', 'counter_technical_id',\n",
    "                            'coordinates',\n",
    "                            'Station Number', 'Measurement Period Duration',\n",
    "                            'date', 'Date and Time', 'counter_installation_date',\n",
    "                    ])\n",
    "\n",
    "\n",
    "\n",
    "y = merged_data['log_bike_count']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the column transformer with OneHotEncoder for 'counter_name' and SimpleImputer for numerical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='mean'), X.select_dtypes(include=['float64', 'int64']).columns),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['counter_name'])\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with the preprocessor, standard scaler, and ElasticNet regression\n",
    "elasticnet_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', ElasticNet(alpha=0.1, l1_ratio=0.5))\n",
    "])\n",
    "\n",
    "# Fit the ElasticNet pipeline on the training data\n",
    "elasticnet_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Print the score of the ElasticNet model on the test data\n",
    "print(f\"ElasticNet model score: {elasticnet_pipeline.score(X_test, y_test)}\")\n",
    "\n",
    "# Output information about the ElasticNet model\n",
    "elasticnet_coefficients = elasticnet_pipeline.named_steps['regressor'].coef_\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "feature_names = (elasticnet_pipeline.named_steps['preprocessor']\n",
    "                 .transformers_[0][2].tolist() +  # numerical features\n",
    "                 elasticnet_pipeline.named_steps['preprocessor']\n",
    "                 .transformers_[1][1].get_feature_names_out(['counter_name']).tolist())  # one-hot encoded features\n",
    "elasticnet_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', ElasticNet(alpha=0.1, l1_ratio=0.5))\n",
    "])\n",
    "\n",
    "# Fit the ElasticNet pipeline on the training data\n",
    "elasticnet_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Print the score of the ElasticNet model on the test data\n",
    "print(f\"ElasticNet model score: {elasticnet_pipeline.score(X_test, y_test)}\")\n",
    "\n",
    "# Output information about the ElasticNet model\n",
    "elasticnet_coefficients = elasticnet_pipeline.named_steps['regressor'].coef_\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "feature_names = (elasticnet_pipeline.named_steps['preprocessor']\n",
    "                 .transformers_[0][2].tolist() +  # numerical features\n",
    "                 elasticnet_pipeline.named_steps['preprocessor']\n",
    "                 .transformers_[1][1].get_feature_names_out(['counter_name']).tolist())  # one-hot encoded features\n",
    "elasticnet_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', ElasticNet(alpha=0.1, l1_ratio=0.5))\n",
    "])\n",
    "\n",
    "# Fit the ElasticNet pipeline on the training data\n",
    "elasticnet_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Print the score of the ElasticNet model on the test data\n",
    "print(f\"ElasticNet model score: {elasticnet_pipeline.score(X_test, y_test)}\")\n",
    "\n",
    "# Output information about the ElasticNet model\n",
    "elasticnet_coefficients = elasticnet_pipeline.named_steps['regressor'].coef_\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "feature_names = (elasticnet_pipeline.named_steps['preprocessor']\n",
    "                 .transformers_[0][2].tolist() +  # numerical features\n",
    "                 elasticnet_pipeline.named_steps['preprocessor']\n",
    "                 .transformers_[1][1].get_feature_names_out(['counter_name']).tolist())  # one-hot encoded features\n",
    "elasticnet_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', ElasticNet(alpha=0.1, l1_ratio=0.5))\n",
    "])\n",
    "\n",
    "# Fit the ElasticNet pipeline on the training data\n",
    "elasticnet_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Print the score of the ElasticNet model on the test data\n",
    "print(f\"ElasticNet model score: {elasticnet_pipeline.score(X_test, y_test)}\")\n",
    "\n",
    "# Output information about the ElasticNet model\n",
    "elasticnet_coefficients = elasticnet_pipeline.named_steps['regressor'].coef_\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "feature_names = (elasticnet_pipeline.named_steps['preprocessor']\n",
    "                 .transformers_[0][2].tolist() +  # numerical features\n",
    "                 elasticnet_pipeline.named_steps['preprocessor']\n",
    "                 .transformers_[1][1].get_feature_names_out(['counter_name']).tolist())  # one-hot encoded features\n",
    "\n",
    "elasticnet_feature_importance = pd.Series(elasticnet_coefficients, index=feature_names).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the feature importances\n",
    "print(elasticnet_feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter features with non-zero importance\n",
    "non_zero_features = elasticnet_feature_importance[elasticnet_feature_importance != 0].index.tolist()\n",
    "# Keep only the non-zero features in the merged dataset\n",
    "# Keep all the variables apart from the columns which are derived from a one hot encoder\n",
    "non_zero_features = [feature for feature in non_zero_features if not feature.startswith('counter_name_')]\n",
    "merged_data_filtered = merged_data[['counter_name', 'bike_count', 'log_bike_count'] + non_zero_features]\n",
    "test_merged_data_filtered = test_merged_data[['counter_name'] + non_zero_features]\n",
    "\n",
    "\n",
    "# Display the new dataframe\n",
    "merged_data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract 273 from all values in the \"Air Temperature (°C)\" column\n",
    "merged_data_filtered.loc[:,'Air Temperature (°C)'] -= 273\n",
    "test_merged_data_filtered.loc[:,'Air Temperature (°C)'] -= 273\n",
    "merged_data_filtered\n",
    "test_merged_data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Define the features and target variable\n",
    "X = merged_data_filtered.drop(columns=[\n",
    "                            'bike_count', 'log_bike_count',\n",
    "                    ])\n",
    "\n",
    "y = merged_data_filtered['log_bike_count']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the column transformer with OneHotEncoder for 'counter_name' and SimpleImputer for numerical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='mean'), X.select_dtypes(include=['float64', 'int64']).columns),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['counter_name'])\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with the preprocessor, standard scaler (with_mean=False), and XGBRegressor\n",
    "xgboostpipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ('regressor', XGBRegressor())\n",
    "])\n",
    "\n",
    "# Fit the XGBRegressor pipeline on the training data\n",
    "xgboostpipeline.fit(X_train, y_train)\n",
    "\n",
    "# Print the score of the XGBRegressor model on the test data\n",
    "print(f\"XGBRegressor model score: {xgboostpipeline.score(X_test, y_test)}\")\n",
    "\n",
    "# Output information about the XGBRegressor model\n",
    "xgboost_feature_importances = xgboostpipeline.named_steps['regressor'].feature_importances_\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "feature_names = (xgboostpipeline.named_steps['preprocessor']\n",
    "                 .transformers_[0][2].tolist() +  # numerical features\n",
    "                 xgboostpipeline.named_steps['preprocessor']\n",
    "                 .transformers_[1][1].get_feature_names_out(['counter_name']).tolist())  # one-hot encoded features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the log_bike_count for the test_merged_data_filtered dataframe\n",
    "y_pred = xgboostpipeline.predict(test_merged_data_filtered)\n",
    "\n",
    "# Display the dataframe with predictions\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'log_bike_count': y_pred\n",
    "}).reset_index(drop=True)\n",
    "submission.index.name = 'Id'\n",
    "\n",
    "submission.to_csv('/Users/felix/Downloads/test.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312-myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
