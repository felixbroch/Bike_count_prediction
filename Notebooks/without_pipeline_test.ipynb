{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison Without Pipeline\n",
    "\n",
    "This notebook compares different machine learning models using manual preprocessing and hyperparameter optimization with Optuna.\n",
    "\n",
    "## Models Tested\n",
    "- **LightGBM**: Gradient boosting framework\n",
    "- **XGBoost**: Extreme gradient boosting\n",
    "- **Random Forest**: Ensemble of decision trees  \n",
    "- **H2O AutoML**: Automated machine learning framework\n",
    "\n",
    "## Features\n",
    "- Manual preprocessing with custom feature engineering\n",
    "- Optuna hyperparameter optimization\n",
    "- Model comparison and selection\n",
    "- Performance evaluation across different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "import pandas as pd\n",
    "from skrub import TableReport, TableVectorizer\n",
    "from jours_feries_france import JoursFeries\n",
    "from vacances_scolaires_france import SchoolHolidayDates\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felix/Python_Data_Challenge-1/utils.py:217: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "  (non_nan_values - value).abs().argmin()\n",
      "/Users/felix/Python_Data_Challenge-1/utils.py:217: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "  (non_nan_values - value).abs().argmin()\n",
      "/Users/felix/Python_Data_Challenge-1/utils.py:383: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[\"log_bike_count\"][\n",
      "/Users/felix/Python_Data_Challenge-1/utils.py:383: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"log_bike_count\"][\n",
      "/Users/felix/Python_Data_Challenge-1/utils.py:388: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[\"log_bike_count\"][\n",
      "/Users/felix/Python_Data_Challenge-1/utils.py:388: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"log_bike_count\"][\n",
      "/Users/felix/Python_Data_Challenge-1/utils.py:393: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[\"log_bike_count\"][\n",
      "/Users/felix/Python_Data_Challenge-1/utils.py:393: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"log_bike_count\"][\n",
      "/Users/felix/Python_Data_Challenge-1/utils.py:398: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[\"log_bike_count\"][\n",
      "/Users/felix/Python_Data_Challenge-1/utils.py:398: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"log_bike_count\"][\n",
      "/Users/felix/Python_Data_Challenge-1/utils.py:403: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[\"log_bike_count\"][\n",
      "/Users/felix/Python_Data_Challenge-1/utils.py:403: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"log_bike_count\"][\n",
      "/Users/felix/Python_Data_Challenge-1/utils.py:408: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[\"log_bike_count\"][\n",
      "/Users/felix/Python_Data_Challenge-1/utils.py:408: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"log_bike_count\"][\n"
     ]
    }
   ],
   "source": [
    "X, y, X_test = utils.get_and_process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2024/felix.brochier/Python_Data_Challenge-1/.venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Preprocess the dataset using TableVectorizer\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_bike_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbike_count\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_bike_count\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Split the data into training and validation sets based on the last 10% of dates\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets based on the last 10% of dates\n",
    "validation_split_index = int(len(X) * 0.9)\n",
    "X_train, X_val = X.iloc[:validation_split_index], X.iloc[validation_split_index:]\n",
    "y_train, y_val = y.iloc[:validation_split_index], y.iloc[validation_split_index:]\n",
    "\n",
    "# Initialize the TableVectorizer\n",
    "vectorizer = TableVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "X_val_transformed = vectorizer.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used the GPU of Ecole Polytechnique to accelerate the tuning of certain models such as XGBoost or LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define GPU-enabled regressors\n",
    "xgb_regressor = XGBRegressor(tree_method='gpu_hist', gpu_id=0, random_state=42)\n",
    "lgbm_regressor = LGBMRegressor(device='gpu', gpu_device_id=0, random_state=42)\n",
    "\n",
    "# Define CPU-based Random Forest\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Columns to exclude from scaling\n",
    "exclude_columns = [\"is_weekend\", \"is_school_holiday\", \"road_work\", \"confinement\", \"couvre_feu\"]\n",
    "numerical_features = [\n",
    "    col for col in X.columns if col not in exclude_columns + [\"counter_name\"]\n",
    "]\n",
    "\n",
    "# Preprocessing: One-Hot Encode \"counter_name\" and scale numerical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),  # Scale numerical columns\n",
    "        ('cat', OneHotEncoder(), ['counter_name'])      # One-hot encode \"counter_name\"\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep other columns unchanged\n",
    ")\n",
    "\n",
    "# Define the Voting Regressor\n",
    "voting_regressor = VotingRegressor([\n",
    "    ('xgb', xgb_regressor),\n",
    "    ('lgbm', lgbm_regressor),\n",
    "    ('rf', rf_regressor)\n",
    "])\n",
    "\n",
    "# Complete Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),  # Preprocessing step\n",
    "    ('voting_regressor', voting_regressor)  # Voting regressor\n",
    "])\n",
    "\n",
    "# # Fit the pipeline\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried using AutoML H2O and accelerate them with the GPU to see if we could get any insights or improvements in our predictions, however, we were not able to make it run quickly in the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from flaml import AutoML\n",
    "from skrub import TableVectorizer\n",
    "\n",
    "\n",
    "# Preprocess the dataset\n",
    "X = df.drop(columns=['log_bike_count', 'bike_count'])\n",
    "y = df['log_bike_count']\n",
    "\n",
    "# Split the data into training and validation sets based on the last 10% of dates\n",
    "validation_split_index = int(len(df) * 0.9)\n",
    "X_train, X_val = X.iloc[:validation_split_index], X.iloc[validation_split_index:]\n",
    "y_train, y_val = y.iloc[:validation_split_index], y.iloc[validation_split_index:]\n",
    "\n",
    "# Initialize the TableVectorizer\n",
    "vectorizer = TableVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "X_val_transformed = vectorizer.transform(X_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from ngboost import NGBRegressor\n",
    "from h2o.automl import H2OAutoML\n",
    "import h2o\n",
    "\n",
    "# Initialize H2O\n",
    "h2o.init()\n",
    "\n",
    "# Dictionary to store the best parameters for each model\n",
    "best_params = {}\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Model selection\n",
    "    model_name = trial.suggest_categorical(\"model\", [\"RandomForest\", \"NGBoost\", \"H2OAutoML\"])\n",
    "    \n",
    "    if model_name == \"RandomForest\":\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 2, 32)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=42,\n",
    "        )\n",
    "        model.fit(X_train_transformed, y_train)\n",
    "        y_pred = model.predict(X_val_transformed)\n",
    "    \n",
    "    elif model_name == \"NGBoost\":\n",
    "        learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-1)\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "        model = NGBRegressor(\n",
    "            learning_rate=learning_rate,\n",
    "            n_estimators=n_estimators,\n",
    "            random_state=42,\n",
    "        )\n",
    "        model.fit(X_train_transformed, y_train)\n",
    "        y_pred = model.predict(X_val_transformed)\n",
    "    \n",
    "    elif model_name == \"H2OAutoML\":\n",
    "        # Convert datasets to H2O frames\n",
    "        train = h2o.H2OFrame(pd.concat([X_train, y_train], axis=1))\n",
    "        val = h2o.H2OFrame(pd.concat([X_val, y_val], axis=1))\n",
    "        \n",
    "        # Specify predictors and response column\n",
    "        predictors = X_train.columns.tolist()\n",
    "        response = \"log_bike_count\"  # Update with your target column name\n",
    "        \n",
    "        # Run H2O AutoML\n",
    "        automl = H2OAutoML(max_models=10, seed=42, nfolds=3)\n",
    "        automl.train(x=predictors, y=response, training_frame=train)\n",
    "        \n",
    "        # Predict on validation set\n",
    "        y_pred = automl.leader.predict(val).as_data_frame()[\"predict\"].values\n",
    "\n",
    "    # Compute the Mean Squared Error (MSE)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    return mse\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Get the best trial and parameters\n",
    "best_trial = study.best_trial\n",
    "best_model_params = study.best_params\n",
    "print(\"Best Trial:\", best_trial)\n",
    "print(\"Best Model Parameters:\", best_model_params)\n",
    "\n",
    "# Save the best model\n",
    "model_name = best_model_params[\"model\"]\n",
    "if model_name == \"H2OAutoML\":\n",
    "    # Save H2O AutoML model\n",
    "    automl.leader.save_mojo(f\"best_{model_name}.mojo\")\n",
    "    print(f\"Best H2O AutoML model saved as 'best_{model_name}.mojo'\")\n",
    "else:\n",
    "    # Save sklearn or NGBoost models\n",
    "    joblib.dump(best_model, f\"best_{model_name}.joblib\")\n",
    "    print(f\"Best model saved as 'best_{model_name}.joblib'\")\n",
    "\n",
    "# Shut down H2O\n",
    "h2o.shutdown(prompt=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
