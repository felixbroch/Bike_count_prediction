{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "import pandas as pd\n",
    "from skrub import TableReport\n",
    "from jours_feries_france import JoursFeries\n",
    "from vacances_scolaires_france import SchoolHolidayDates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"data/train.parquet\")\n",
    "# Sort by date first, so that time based cross-validation would produce correct results\n",
    "data = data.sort_values([\"date\", \"counter_name\"])\n",
    "\n",
    "data_test = pd.read_parquet(\"data/final_test.parquet\")\n",
    "# Sort by date first, so that time based cross-validation would produce correct results\n",
    "data_test = data_test.sort_values([\"date\", \"counter_name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_conditions = pd.read_csv('data/external_data.csv')\n",
    "external_conditions['date'] = pd.to_datetime(external_conditions['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with more than 40% NaN values\n",
    "threshold = len(external_conditions) * 0.4\n",
    "external_conditions = external_conditions.dropna(thresh=threshold, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28392/3528442862.py:34: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "  closest_value = non_nan_values.iloc[(non_nan_values - value).abs().argmin()]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Sort the `external_conditions` DataFrame by the `date` column\n",
    "external_conditions = external_conditions.sort_values(by='date')\n",
    "\n",
    "# Drop columns with more than 40% NaN values\n",
    "threshold = len(external_conditions) * 0.4\n",
    "external_conditions = external_conditions.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "# Step 2: Remove duplicate entries based on the `date` column\n",
    "external_conditions = external_conditions.drop_duplicates(subset='date')\n",
    "\n",
    "# Step 3: Convert the 'date' column to datetime\n",
    "external_conditions['date'] = pd.to_datetime(external_conditions['date'])\n",
    "\n",
    "# Step 4: Create a complete date range from the minimum to the maximum date in the DataFrame\n",
    "date_range = pd.date_range(start=external_conditions['date'].min(), end=external_conditions['date'].max(), freq='h')\n",
    "\n",
    "# Step 5: Create a DataFrame from the date_range\n",
    "date_range_df = pd.DataFrame(date_range, columns=['date'])\n",
    "\n",
    "# Step 6: Merge the date_range DataFrame with the external_conditions DataFrame on the 'date' column\n",
    "full_external_conditions = pd.merge(date_range_df, external_conditions, on='date', how='left')\n",
    "\n",
    "# Fonction qui fait ce qu'on voulait faire avec ffill et bfill mais a la place prends la valeur la plus proche\n",
    "def fill_closest_value_all_columns(df):\n",
    "    \"\"\"Fill NaN values with the closest value for all numeric columns in the DataFrame.\"\"\"\n",
    "    filled_df = df.copy()\n",
    "    \n",
    "    for column in filled_df.columns:\n",
    "        if filled_df[column].dtype.kind in 'biufc':  # Numeric columns\n",
    "            non_nan_values = filled_df[column].dropna()\n",
    "            \n",
    "            def find_closest(value):\n",
    "                if pd.isna(value):\n",
    "                    closest_value = non_nan_values.iloc[(non_nan_values - value).abs().argmin()]\n",
    "                    return closest_value\n",
    "                return value\n",
    "            \n",
    "            filled_df[column] = filled_df[column].apply(find_closest)\n",
    "    \n",
    "    return filled_df\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "filled_external_conditions = fill_closest_value_all_columns(full_external_conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'date' column in external_conditions is of datetime type\n",
    "# filled_external_conditions['date'] = pd.to_datetime(filled_external_conditions['date'])\n",
    "\n",
    "# Merge the DataFrames\n",
    "merged_conditions = pd.merge(data, filled_external_conditions, on='date', how='left')\n",
    "\n",
    "merged_conditions = utils._column_rename(merged_conditions)\n",
    "\n",
    "\n",
    "merged_conditions_test = pd.merge(data_test, filled_external_conditions, on='date', how='left')\n",
    "\n",
    "merged_conditions_test = utils._column_rename(merged_conditions_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure \"date\" is in datetime format\n",
    "merged_conditions[\"date\"] = pd.to_datetime(merged_conditions[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with invalid datetime entries\n",
    "df = merged_conditions.dropna(subset=[\"date\"])\n",
    "\n",
    "# Extract date and time features\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "df[\"weekday\"] = df[\"date\"].dt.dayofweek\n",
    "df[\"day\"] = df[\"date\"].dt.day\n",
    "df[\"hour\"] = df[\"date\"].dt.hour\n",
    "df[\"is_weekend\"] = (df[\"weekday\"] >= 5).astype(int)\n",
    "\n",
    "# Handle school and public holidays\n",
    "unique_dates = df[\"date\"].dt.date.unique()\n",
    "d = SchoolHolidayDates()\n",
    "f = JoursFeries()\n",
    "\n",
    "try:\n",
    "    dict_school_holidays = {date: d.is_holiday_for_zone(date, \"C\") for date in unique_dates}\n",
    "    df[\"is_school_holiday\"] = df[\"date\"].dt.date.map(dict_school_holidays).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with school holidays mapping: {e}\")\n",
    "    df[\"is_school_holiday\"] = 0\n",
    "\n",
    "try:\n",
    "    dict_public_holidays = {date: f.is_bank_holiday(date, zone=\"Métropole\") for date in unique_dates}\n",
    "    df[\"is_public_holiday\"] = df[\"date\"].dt.date.map(dict_public_holidays).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with public holidays mapping: {e}\")\n",
    "    df[\"is_public_holiday\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure \"date\" is in datetime format\n",
    "merged_conditions_test[\"date\"] = pd.to_datetime(merged_conditions_test[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with invalid datetime entries\n",
    "df_test = merged_conditions_test.dropna(subset=[\"date\"])\n",
    "\n",
    "# Extract date and time features\n",
    "df_test[\"year\"] = df_test[\"date\"].dt.year\n",
    "df_test[\"month\"] = df_test[\"date\"].dt.month\n",
    "df_test[\"weekday\"] = df_test[\"date\"].dt.dayofweek\n",
    "df_test[\"day\"] = df_test[\"date\"].dt.day\n",
    "df_test[\"hour\"] = df_test[\"date\"].dt.hour\n",
    "df_test[\"is_weekend\"] = (df_test[\"weekday\"] >= 5).astype(int)\n",
    "\n",
    "# Handle school and public holidays\n",
    "unique_dates = df_test[\"date\"].dt.date.unique()\n",
    "d = SchoolHolidayDates()\n",
    "f = JoursFeries()\n",
    "\n",
    "try:\n",
    "    dict_school_holidays = {date: d.is_holiday_for_zone(date, \"C\") for date in unique_dates}\n",
    "    df_test[\"is_school_holiday\"] = df_test[\"date\"].dt.date.map(dict_school_holidays).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with school holidays mapping: {e}\")\n",
    "    df_test[\"is_school_holiday\"] = 0\n",
    "\n",
    "try:\n",
    "    dict_public_holidays = {date: f.is_bank_holiday(date, zone=\"Métropole\") for date in unique_dates}\n",
    "    df_test[\"is_public_holiday\"] = df_test[\"date\"].dt.date.map(dict_public_holidays).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with public holidays mapping: {e}\")\n",
    "    df_test[\"is_public_holiday\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TableReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TableReport(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test using flaml and the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2024/felix.brochier/Python_Data_Challenge-1/.venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from flaml import AutoML\n",
    "from skrub import TableVectorizer\n",
    "\n",
    "\n",
    "# Preprocess the dataset\n",
    "X = df.drop(columns=['log_bike_count', 'bike_count'])\n",
    "y = df['log_bike_count']\n",
    "\n",
    "# Split the data into training and validation sets based on the last 10% of dates\n",
    "validation_split_index = int(len(df) * 0.9)\n",
    "X_train, X_val = X.iloc[:validation_split_index], X.iloc[validation_split_index:]\n",
    "y_train, y_val = y.iloc[:validation_split_index], y.iloc[validation_split_index:]\n",
    "\n",
    "# Initialize the TableVectorizer\n",
    "vectorizer = TableVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "X_val_transformed = vectorizer.transform(X_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"17.0.13\" 2024-10-15 LTS; OpenJDK Runtime Environment (Red_Hat-17.0.13.0.11-1) (build 17.0.13+11-LTS); OpenJDK 64-Bit Server VM (Red_Hat-17.0.13.0.11-1) (build 17.0.13+11-LTS, mixed mode, sharing)\n",
      "  Starting server from /users/eleves-a/2024/felix.brochier/Python_Data_Challenge-1/.venv/lib/python3.9/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmp9573vxfe\n",
      "  JVM stdout: /tmp/tmp9573vxfe/h2o_felix_brochier_started_from_python.out\n",
      "  JVM stderr: /tmp/tmp9573vxfe/h2o_felix_brochier_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-1.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-1 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-1 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-1 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table th,\n",
       "#h2o-table-1 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Paris</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.46.0.6</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>1 month and 7 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_felix_brochier_tqgfan</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>15.58 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.9.19 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  -------------------------------------\n",
       "H2O_cluster_uptime:         01 secs\n",
       "H2O_cluster_timezone:       Europe/Paris\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.46.0.6\n",
       "H2O_cluster_version_age:    1 month and 7 days\n",
       "H2O_cluster_name:           H2O_from_python_felix_brochier_tqgfan\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    15.58 Gb\n",
       "H2O_cluster_total_cores:    16\n",
       "H2O_cluster_allowed_cores:  16\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.9.19 final\n",
       "--------------------------  -------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 16:38:22,517] A new study created in memory with name: no-name-2918b00c-3b07-487a-b503-ef2acb163564\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "# Initialize H2O\n",
    "h2o.init()\n",
    "\n",
    "# Dictionary to store the best parameters for each model\n",
    "best_params = {}\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Model selection\n",
    "    model_name = trial.suggest_categorical(\"model\", [\"RandomForest\", \"LightGBM\", \"H2OAutoML\"])\n",
    "    \n",
    "    if model_name == \"RandomForest\":\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 2, 32)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=42,\n",
    "        )\n",
    "        model.fit(X_train_transformed, y_train)\n",
    "        y_pred = model.predict(X_val_transformed)\n",
    "    \n",
    "    elif model_name == \"LightGBM\":\n",
    "        num_leaves = trial.suggest_int(\"num_leaves\", 31, 256)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", -1, 32)  # -1 means no limit\n",
    "        learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-1)\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "        model = lgb.LGBMRegressor(\n",
    "            num_leaves=num_leaves,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            n_estimators=n_estimators,\n",
    "            random_state=42,\n",
    "        )\n",
    "        model.fit(X_train_transformed, y_train)\n",
    "        y_pred = model.predict(X_val_transformed)\n",
    "    \n",
    "    elif model_name == \"H2OAutoML\":\n",
    "        # Convert datasets to H2O frames\n",
    "        train = h2o.H2OFrame(pd.concat([X_train, y_train], axis=1))\n",
    "        val = h2o.H2OFrame(pd.concat([X_val, y_val], axis=1))\n",
    "        \n",
    "        # Specify predictors and response column\n",
    "        predictors = X_train.columns.tolist()\n",
    "        response = \"log_bike_count\"  # Update with your target column name\n",
    "        \n",
    "        # Run H2O AutoML\n",
    "        automl = H2OAutoML(max_models=10, seed=42, nfolds=3)\n",
    "        automl.train(x=predictors, y=response, training_frame=train)\n",
    "        \n",
    "        # Predict on validation set\n",
    "        y_pred = automl.leader.predict(val).as_data_frame()[\"predict\"].values\n",
    "\n",
    "    # Compute the Mean Squared Error (MSE)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    return mse\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Get the best trial and parameters\n",
    "best_trial = study.best_trial\n",
    "best_model_params = study.best_params\n",
    "print(\"Best Trial:\", best_trial)\n",
    "print(\"Best Model Parameters:\", best_model_params)\n",
    "\n",
    "# Save the best model\n",
    "model_name = best_model_params[\"model\"]\n",
    "if model_name == \"H2OAutoML\":\n",
    "    # Save H2O AutoML model\n",
    "    automl.leader.save_mojo(f\"best_{model_name}.mojo\")\n",
    "    print(f\"Best H2O AutoML model saved as 'best_{model_name}.mojo'\")\n",
    "else:\n",
    "    # Save sklearn or LightGBM models\n",
    "    joblib.dump(best_model, f\"best_{model_name}.joblib\")\n",
    "    print(f\"Best model saved as 'best_{model_name}.joblib'\")\n",
    "\n",
    "# Shut down H2O\n",
    "h2o.shutdown(prompt=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test using Optuna hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import TableVectorizer\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preprocess the dataset using TableVectorizer\n",
    "X = df.drop(columns=['log_bike_count', 'bike_count'])\n",
    "y = df['log_bike_count']\n",
    "\n",
    "# # Split the data into training and validation sets based on the last 10% of dates\n",
    "# validation_split_index = int(len(df) * 0.9)\n",
    "# X_train, X_val = X.iloc[:validation_split_index], X.iloc[validation_split_index:]\n",
    "# y_train, y_val = y.iloc[:validation_split_index], y.iloc[validation_split_index:]\n",
    "\n",
    "# Initialize the TableVectorizer\n",
    "# vectorizer = TableVectorizer()\n",
    "\n",
    "# # Fit and transform the training data\n",
    "# X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "# X_val_transformed = vectorizer.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the objective function for Optuna\n",
    "# def objective(trial):\n",
    "#     param = {\n",
    "#         'objective': 'reg:squarederror',\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.1, 0.2),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "#         'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n",
    "#         'random_state': 42,\n",
    "#         'tree_method': 'gpu_hist',  # Enable GPU support\n",
    "#         'predictor': 'gpu_predictor'\n",
    "#     }\n",
    "#     model = XGBRegressor(**param)\n",
    "#     model.fit(X_train_transformed, y_train)\n",
    "#     return model.score(X_val_transformed, y_val)  # Maximizing validation R² score\n",
    "\n",
    "# # Create a study and optimize the objective function\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=50)\n",
    "\n",
    "# # Get the best parameters\n",
    "# best_params = study.best_params\n",
    "\n",
    "# # Add GPU-specific parameter to the best parameters\n",
    "# best_params['tree_method'] = 'gpu_hist'  # Ensure GPU is used for the final model\n",
    "\n",
    "# # Train the final model with the best parameters\n",
    "# X_transformed = vectorizer.transform(X)  # Transform the entire dataset\n",
    "# final_model = XGBRegressor(**best_params)\n",
    "# final_model.fit(X_transformed, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from xgboost import XGBRegressor\n",
    "from skrub import TableVectorizer\n",
    "\n",
    "# Load the best parameters from the pickle file\n",
    "best_params = joblib.load('xg_boost_best_params.pkl')\n",
    "\n",
    "# Check if GPU support is available\n",
    "try:\n",
    "    import xgboost\n",
    "    if 'gpu_hist' in best_params.get('tree_method', '') and not xgboost.Booster().attr('gpu_id'):\n",
    "        print(\"Warning: XGBoost is not compiled with GPU support. Falling back to CPU.\")\n",
    "        best_params.pop('tree_method', None)  # Remove GPU-specific parameters\n",
    "        best_params.pop('predictor', None)\n",
    "except Exception as e:\n",
    "    print(f\"Error while checking GPU support: {e}\")\n",
    "\n",
    "# Initialize the TableVectorizer\n",
    "vectorizer = TableVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_transformed = vectorizer.fit_transform(X)\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "final_model = XGBRegressor(**best_params)\n",
    "final_model.fit(X_transformed, y)\n",
    "\n",
    "# Print model parameters\n",
    "print(\"Trained model parameters:\")\n",
    "print(final_model.get_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test data using the same vectorizer instance\n",
    "X_test_transformed = vectorizer.transform(df_test)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = final_model.predict(X_test_transformed)\n",
    "\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(y_pred, columns=[\"log_bike_count\"])\n",
    "df_submission.index = data_test.index\n",
    "df_submission.index.name = \"Id\"\n",
    "df_submission.to_csv(\"/Users/felix/Documents/X/Cours Python/Kaggle/submission/test_pipeline.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_parquet('data/final_test.parquet')\n",
    "# Merge the DataFrames\n",
    "merged_conditions = pd.merge(test_data, filled_external_conditions, on='date', how='left')\n",
    "\n",
    "merged_conditions = utils._column_rename(merged_conditions)\n",
    "\n",
    "# Ensure \"date\" is in datetime format\n",
    "merged_conditions[\"date\"] = pd.to_datetime(merged_conditions[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with invalid datetime entries\n",
    "df_test = merged_conditions.dropna(subset=[\"date\"])\n",
    "\n",
    "# Extract date and time features\n",
    "df_test[\"year\"] = df_test[\"date\"].dt.year\n",
    "df_test[\"month\"] = df_test[\"date\"].dt.month\n",
    "df_test[\"weekday\"] = df_test[\"date\"].dt.dayofweek\n",
    "df_test[\"day\"] = df_test[\"date\"].dt.day\n",
    "df_test[\"hour\"] = df_test[\"date\"].dt.hour\n",
    "df_test[\"is_weekend\"] = (df_test[\"weekday\"] >= 5).astype(int)\n",
    "\n",
    "# Handle school and public holidays\n",
    "unique_dates_test = df_test[\"date\"].dt.date.unique()\n",
    "\n",
    "try:\n",
    "    dict_school_holidays_test = {date: d.is_holiday_for_zone(date, \"C\") for date in unique_dates_test}\n",
    "    df_test[\"is_school_holiday\"] = df_test[\"date\"].dt.date.map(dict_school_holidays_test).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with school holidays mapping: {e}\")\n",
    "    df_test[\"is_school_holiday\"] = 0\n",
    "\n",
    "try:\n",
    "    dict_public_holidays_test = {date: f.is_bank_holiday(date, zone=\"Métropole\") for date in unique_dates_test}\n",
    "    df_test[\"is_public_holiday\"] = df_test[\"date\"].dt.date.map(dict_public_holidays_test).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with public holidays mapping: {e}\")\n",
    "    df_test[\"is_public_holiday\"] = 0\n",
    "\n",
    "# Predict using the pipeline\n",
    "y_pred_test = pipeline.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(y_pred_test, columns=[\"log_bike_count\"])\n",
    "df_submission.index.name = \"Id\"\n",
    "df_submission\n",
    "df_submission.to_csv(\"/Users/felix/Documents/X/Cours Python/Kaggle/submission/test_pipeline.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En dessous c'est des tests d'avant ca ne fait pas tourner ce qui marche actuellement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new category to categorical columns\n",
    "for col in df.select_dtypes(include=['category']).columns:\n",
    "\tdf[col] = df[col].cat.add_categories([0])\n",
    "\n",
    "# Fill NaN values with 0\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df['log_bike_count'].values\n",
    "X_train = df.drop(['log_bike_count', \"bike_count\"], axis=1)\n",
    "\n",
    "date_cols = [\"year\", \"month\", \"weekday\", \"day\", \"hour\", \"is_weekend\", \"is_school_holiday\", \"is_public_holiday\"]\n",
    "categorical_cols = [\"counter_name\"]\n",
    "numerical_cols = [\n",
    "    'latitude', 'longitude', 'Sea Level Pressure (hPa)', 'Pressure Tendency (hPa/3h)',\n",
    "    'Pressure Tendency Code', 'Wind Direction (°)', 'Wind Speed (m/s)', 'Air Temperature (°C)',\n",
    "    'Dew Point Temperature (°C)', 'Relative Humidity (%)', 'Visibility (m)', 'Present Weather Code',\n",
    "    'Past Weather Code 1', 'Past Weather Code 2', 'Total Cloud Cover (oktas)', 'Cloud Base Height (m)',\n",
    "    'Lowest Cloud Base Height (m)', 'Low Cloud Type', 'Station Level Pressure (hPa)', '24h Pressure Tendency (hPa)',\n",
    "    '10min Max Wind Gust (m/s)', 'Max Wind Gust (m/s)', 'Measurement Period Duration', 'Ground State',\n",
    "    'Snow Height (cm)', 'New Snow Depth (cm)', 'New Snowfall Duration (hours)', 'Rainfall (1h, mm)',\n",
    "    'Rainfall (3h, mm)', 'Rainfall (6h, mm)', 'Rainfall (12h, mm)', 'Rainfall (24h, mm)',\n",
    "    'Layer 1 Cloud Cover (oktas)', 'Layer 1 Cloud Type', 'Layer 1 Cloud Base Height (m)'\n",
    "]\n",
    "\n",
    "\n",
    "# 1. Apply column transformations\n",
    "# One-hot encode date columns\n",
    "date_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "date_encoded = date_encoder.fit_transform(X_train[date_cols])\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "cat_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "cat_encoded = cat_encoder.fit_transform(X_train[categorical_cols])\n",
    "\n",
    "# Standard scale numerical columns\n",
    "num_scaler = StandardScaler()\n",
    "num_scaled = num_scaler.fit_transform(X_train[numerical_cols])\n",
    "\n",
    "X_transformed = hstack([date_encoded, cat_encoded, num_scaled]).toarray()\n",
    "\n",
    "# 2. Train the model\n",
    "model = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_transformed, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = utils.get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames\n",
    "merged_conditions = pd.merge(X_test, external_conditions, on='date', how='left')\n",
    "\n",
    "merged_conditions = utils._column_rename(merged_conditions)\n",
    "# Ensure \"date\" is in datetime format\n",
    "merged_conditions[\"date\"] = pd.to_datetime(merged_conditions[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with invalid datetime entries\n",
    "df = merged_conditions.dropna(subset=[\"date\"])\n",
    "\n",
    "# Extract date and time features\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "df[\"weekday\"] = df[\"date\"].dt.dayofweek\n",
    "df[\"day\"] = df[\"date\"].dt.day\n",
    "df[\"hour\"] = df[\"date\"].dt.hour\n",
    "df[\"is_weekend\"] = (df[\"weekday\"] >= 5).astype(int)\n",
    "\n",
    "# Handle school and public holidays\n",
    "unique_dates = df[\"date\"].dt.date.unique()\n",
    "d = SchoolHolidayDates()\n",
    "f = JoursFeries()\n",
    "\n",
    "try:\n",
    "    dict_school_holidays = {date: d.is_holiday_for_zone(date, \"C\") for date in unique_dates}\n",
    "    df[\"is_school_holiday\"] = df[\"date\"].dt.date.map(dict_school_holidays).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with school holidays mapping: {e}\")\n",
    "    df[\"is_school_holiday\"] = 0\n",
    "\n",
    "try:\n",
    "    dict_public_holidays = {date: f.is_bank_holiday(date, zone=\"Métropole\") for date in unique_dates}\n",
    "    df[\"is_public_holiday\"] = df[\"date\"].dt.date.map(dict_public_holidays).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with public holidays mapping: {e}\")\n",
    "    df[\"is_public_holiday\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the test data with the same transformations as the training data\n",
    "# 1. Apply column transformations\n",
    "# One-hot encode date columns\n",
    "date_encoded_test = date_encoder.transform(df[date_cols])\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "cat_encoded_test = cat_encoder.transform(df[categorical_cols])\n",
    "\n",
    "# Standard scale numerical columns\n",
    "num_scaled_test = num_scaler.transform(df[numerical_cols])\n",
    "\n",
    "# Combine all transformed features\n",
    "X_test_transformed_numeric = hstack([date_encoded_test, cat_encoded_test, num_scaled_test]).toarray()\n",
    "\n",
    "# 2. Make predictions\n",
    "y_pred = model.predict(X_test_transformed_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(y_pred, columns=[\"log_bike_count\"])\n",
    "df_submission.index.name = \"Id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv(\"/Users/felix/Documents/X/Cours Python/Kaggle/submission/test_pipeline.csv\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
