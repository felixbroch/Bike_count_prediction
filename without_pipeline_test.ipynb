{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.sparse import hstack\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skrub import TableReport\n",
    "from jours_feries_france import JoursFeries\n",
    "from vacances_scolaires_france import SchoolHolidayDates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"data/train.parquet\")\n",
    "# Sort by date first, so that time based cross-validation would produce correct results\n",
    "data = data.sort_values([\"date\", \"counter_name\"])\n",
    "\n",
    "data_test = pd.read_parquet(\"data/final_test.parquet\")\n",
    "# Sort by date first, so that time based cross-validation would produce correct results\n",
    "data_test = data_test.sort_values([\"date\", \"counter_name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counter_id</th>\n",
       "      <th>counter_name</th>\n",
       "      <th>site_id</th>\n",
       "      <th>site_name</th>\n",
       "      <th>date</th>\n",
       "      <th>counter_installation_date</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>counter_technical_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17081</th>\n",
       "      <td>100049407-353255860</td>\n",
       "      <td>152 boulevard du Montparnasse E-O</td>\n",
       "      <td>100049407</td>\n",
       "      <td>152 boulevard du Montparnasse</td>\n",
       "      <td>2021-09-10 01:00:00</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>48.840801,2.333233</td>\n",
       "      <td>Y2H19070373</td>\n",
       "      <td>48.840801</td>\n",
       "      <td>2.333233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18655</th>\n",
       "      <td>100049407-353255859</td>\n",
       "      <td>152 boulevard du Montparnasse O-E</td>\n",
       "      <td>100049407</td>\n",
       "      <td>152 boulevard du Montparnasse</td>\n",
       "      <td>2021-09-10 01:00:00</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>48.840801,2.333233</td>\n",
       "      <td>Y2H19070373</td>\n",
       "      <td>48.840801</td>\n",
       "      <td>2.333233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3124</th>\n",
       "      <td>100036719-104036719</td>\n",
       "      <td>18 quai de l'Hôtel de Ville NO-SE</td>\n",
       "      <td>100036719</td>\n",
       "      <td>18 quai de l'Hôtel de Ville</td>\n",
       "      <td>2021-09-10 01:00:00</td>\n",
       "      <td>2017-07-12</td>\n",
       "      <td>48.85372,2.35702</td>\n",
       "      <td>Y2H19027732</td>\n",
       "      <td>48.853720</td>\n",
       "      <td>2.357020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4147</th>\n",
       "      <td>100036719-103036719</td>\n",
       "      <td>18 quai de l'Hôtel de Ville SE-NO</td>\n",
       "      <td>100036719</td>\n",
       "      <td>18 quai de l'Hôtel de Ville</td>\n",
       "      <td>2021-09-10 01:00:00</td>\n",
       "      <td>2017-07-12</td>\n",
       "      <td>48.85372,2.35702</td>\n",
       "      <td>Y2H19027732</td>\n",
       "      <td>48.853720</td>\n",
       "      <td>2.357020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48210</th>\n",
       "      <td>100063175-353277233</td>\n",
       "      <td>20 Avenue de Clichy NO-SE</td>\n",
       "      <td>100063175</td>\n",
       "      <td>20 Avenue de Clichy</td>\n",
       "      <td>2021-09-10 01:00:00</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>48.88529,2.32666</td>\n",
       "      <td>Y2H20073268</td>\n",
       "      <td>48.885290</td>\n",
       "      <td>2.326660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42131</th>\n",
       "      <td>100057329-103057329</td>\n",
       "      <td>Totem 85 quai d'Austerlitz SE-NO</td>\n",
       "      <td>100057329</td>\n",
       "      <td>Totem 85 quai d'Austerlitz</td>\n",
       "      <td>2021-10-18 21:00:00</td>\n",
       "      <td>2020-02-18</td>\n",
       "      <td>48.84201,2.36729</td>\n",
       "      <td>YTH19111508</td>\n",
       "      <td>48.842010</td>\n",
       "      <td>2.367290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43042</th>\n",
       "      <td>100057380-104057380</td>\n",
       "      <td>Totem Cours la Reine E-O</td>\n",
       "      <td>100057380</td>\n",
       "      <td>Totem Cours la Reine</td>\n",
       "      <td>2021-10-18 21:00:00</td>\n",
       "      <td>2020-02-11</td>\n",
       "      <td>48.86462,2.31444</td>\n",
       "      <td>YTH19111509</td>\n",
       "      <td>48.864620</td>\n",
       "      <td>2.314440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43929</th>\n",
       "      <td>100057380-103057380</td>\n",
       "      <td>Totem Cours la Reine O-E</td>\n",
       "      <td>100057380</td>\n",
       "      <td>Totem Cours la Reine</td>\n",
       "      <td>2021-10-18 21:00:00</td>\n",
       "      <td>2020-02-11</td>\n",
       "      <td>48.86462,2.31444</td>\n",
       "      <td>YTH19111509</td>\n",
       "      <td>48.864620</td>\n",
       "      <td>2.314440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5707</th>\n",
       "      <td>100042374-110042374</td>\n",
       "      <td>Voie Georges Pompidou NE-SO</td>\n",
       "      <td>100042374</td>\n",
       "      <td>Voie Georges Pompidou</td>\n",
       "      <td>2021-10-18 21:00:00</td>\n",
       "      <td>2017-12-15</td>\n",
       "      <td>48.8484,2.27586</td>\n",
       "      <td>Y2H21025335</td>\n",
       "      <td>48.848400</td>\n",
       "      <td>2.275860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6855</th>\n",
       "      <td>100042374-109042374</td>\n",
       "      <td>Voie Georges Pompidou SO-NE</td>\n",
       "      <td>100042374</td>\n",
       "      <td>Voie Georges Pompidou</td>\n",
       "      <td>2021-10-18 21:00:00</td>\n",
       "      <td>2017-12-15</td>\n",
       "      <td>48.8484,2.27586</td>\n",
       "      <td>Y2H21025335</td>\n",
       "      <td>48.848400</td>\n",
       "      <td>2.275860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51440 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                counter_id                       counter_name    site_id  \\\n",
       "17081  100049407-353255860  152 boulevard du Montparnasse E-O  100049407   \n",
       "18655  100049407-353255859  152 boulevard du Montparnasse O-E  100049407   \n",
       "3124   100036719-104036719  18 quai de l'Hôtel de Ville NO-SE  100036719   \n",
       "4147   100036719-103036719  18 quai de l'Hôtel de Ville SE-NO  100036719   \n",
       "48210  100063175-353277233          20 Avenue de Clichy NO-SE  100063175   \n",
       "...                    ...                                ...        ...   \n",
       "42131  100057329-103057329   Totem 85 quai d'Austerlitz SE-NO  100057329   \n",
       "43042  100057380-104057380           Totem Cours la Reine E-O  100057380   \n",
       "43929  100057380-103057380           Totem Cours la Reine O-E  100057380   \n",
       "5707   100042374-110042374        Voie Georges Pompidou NE-SO  100042374   \n",
       "6855   100042374-109042374        Voie Georges Pompidou SO-NE  100042374   \n",
       "\n",
       "                           site_name                date  \\\n",
       "17081  152 boulevard du Montparnasse 2021-09-10 01:00:00   \n",
       "18655  152 boulevard du Montparnasse 2021-09-10 01:00:00   \n",
       "3124     18 quai de l'Hôtel de Ville 2021-09-10 01:00:00   \n",
       "4147     18 quai de l'Hôtel de Ville 2021-09-10 01:00:00   \n",
       "48210            20 Avenue de Clichy 2021-09-10 01:00:00   \n",
       "...                              ...                 ...   \n",
       "42131     Totem 85 quai d'Austerlitz 2021-10-18 21:00:00   \n",
       "43042           Totem Cours la Reine 2021-10-18 21:00:00   \n",
       "43929           Totem Cours la Reine 2021-10-18 21:00:00   \n",
       "5707           Voie Georges Pompidou 2021-10-18 21:00:00   \n",
       "6855           Voie Georges Pompidou 2021-10-18 21:00:00   \n",
       "\n",
       "      counter_installation_date         coordinates counter_technical_id  \\\n",
       "17081                2018-12-07  48.840801,2.333233          Y2H19070373   \n",
       "18655                2018-12-07  48.840801,2.333233          Y2H19070373   \n",
       "3124                 2017-07-12    48.85372,2.35702          Y2H19027732   \n",
       "4147                 2017-07-12    48.85372,2.35702          Y2H19027732   \n",
       "48210                2020-07-22    48.88529,2.32666          Y2H20073268   \n",
       "...                         ...                 ...                  ...   \n",
       "42131                2020-02-18    48.84201,2.36729          YTH19111508   \n",
       "43042                2020-02-11    48.86462,2.31444          YTH19111509   \n",
       "43929                2020-02-11    48.86462,2.31444          YTH19111509   \n",
       "5707                 2017-12-15     48.8484,2.27586          Y2H21025335   \n",
       "6855                 2017-12-15     48.8484,2.27586          Y2H21025335   \n",
       "\n",
       "        latitude  longitude  \n",
       "17081  48.840801   2.333233  \n",
       "18655  48.840801   2.333233  \n",
       "3124   48.853720   2.357020  \n",
       "4147   48.853720   2.357020  \n",
       "48210  48.885290   2.326660  \n",
       "...          ...        ...  \n",
       "42131  48.842010   2.367290  \n",
       "43042  48.864620   2.314440  \n",
       "43929  48.864620   2.314440  \n",
       "5707   48.848400   2.275860  \n",
       "6855   48.848400   2.275860  \n",
       "\n",
       "[51440 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numer_sta</th>\n",
       "      <th>date</th>\n",
       "      <th>pmer</th>\n",
       "      <th>tend</th>\n",
       "      <th>cod_tend</th>\n",
       "      <th>dd</th>\n",
       "      <th>ff</th>\n",
       "      <th>t</th>\n",
       "      <th>td</th>\n",
       "      <th>u</th>\n",
       "      <th>...</th>\n",
       "      <th>hnuage1</th>\n",
       "      <th>nnuage2</th>\n",
       "      <th>ctype2</th>\n",
       "      <th>hnuage2</th>\n",
       "      <th>nnuage3</th>\n",
       "      <th>ctype3</th>\n",
       "      <th>hnuage3</th>\n",
       "      <th>nnuage4</th>\n",
       "      <th>ctype4</th>\n",
       "      <th>hnuage4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7149</td>\n",
       "      <td>2021-01-01 00:00:00</td>\n",
       "      <td>100810</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>270</td>\n",
       "      <td>1.8</td>\n",
       "      <td>272.75</td>\n",
       "      <td>272.15</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>600.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7149</td>\n",
       "      <td>2021-01-01 03:00:00</td>\n",
       "      <td>100920</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>1.7</td>\n",
       "      <td>271.25</td>\n",
       "      <td>270.95</td>\n",
       "      <td>98</td>\n",
       "      <td>...</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7149</td>\n",
       "      <td>2021-01-01 06:00:00</td>\n",
       "      <td>100950</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>290</td>\n",
       "      <td>2.6</td>\n",
       "      <td>271.95</td>\n",
       "      <td>271.65</td>\n",
       "      <td>98</td>\n",
       "      <td>...</td>\n",
       "      <td>480.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7149</td>\n",
       "      <td>2021-01-01 09:00:00</td>\n",
       "      <td>101100</td>\n",
       "      <td>150</td>\n",
       "      <td>2</td>\n",
       "      <td>280</td>\n",
       "      <td>1.7</td>\n",
       "      <td>272.45</td>\n",
       "      <td>272.05</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7149</td>\n",
       "      <td>2021-01-01 12:00:00</td>\n",
       "      <td>101110</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>276.95</td>\n",
       "      <td>274.15</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>330.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>810.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3317</th>\n",
       "      <td>7149</td>\n",
       "      <td>2020-09-30 09:00:00</td>\n",
       "      <td>101540</td>\n",
       "      <td>-30</td>\n",
       "      <td>8</td>\n",
       "      <td>230</td>\n",
       "      <td>4.4</td>\n",
       "      <td>289.95</td>\n",
       "      <td>286.85</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>400.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3318</th>\n",
       "      <td>7149</td>\n",
       "      <td>2020-09-30 12:00:00</td>\n",
       "      <td>101320</td>\n",
       "      <td>-210</td>\n",
       "      <td>8</td>\n",
       "      <td>190</td>\n",
       "      <td>4.9</td>\n",
       "      <td>292.05</td>\n",
       "      <td>285.55</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>870.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3319</th>\n",
       "      <td>7149</td>\n",
       "      <td>2020-09-30 15:00:00</td>\n",
       "      <td>101140</td>\n",
       "      <td>-180</td>\n",
       "      <td>7</td>\n",
       "      <td>190</td>\n",
       "      <td>4.1</td>\n",
       "      <td>291.55</td>\n",
       "      <td>286.45</td>\n",
       "      <td>72</td>\n",
       "      <td>...</td>\n",
       "      <td>820.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3320</th>\n",
       "      <td>7149</td>\n",
       "      <td>2020-09-30 18:00:00</td>\n",
       "      <td>101020</td>\n",
       "      <td>-130</td>\n",
       "      <td>6</td>\n",
       "      <td>190</td>\n",
       "      <td>2.7</td>\n",
       "      <td>290.15</td>\n",
       "      <td>285.25</td>\n",
       "      <td>73</td>\n",
       "      <td>...</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3321</th>\n",
       "      <td>7149</td>\n",
       "      <td>2020-09-30 21:00:00</td>\n",
       "      <td>100880</td>\n",
       "      <td>-140</td>\n",
       "      <td>8</td>\n",
       "      <td>170</td>\n",
       "      <td>2.4</td>\n",
       "      <td>288.35</td>\n",
       "      <td>285.85</td>\n",
       "      <td>85</td>\n",
       "      <td>...</td>\n",
       "      <td>1980.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3322 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      numer_sta                date    pmer  tend  cod_tend   dd   ff       t  \\\n",
       "0          7149 2021-01-01 00:00:00  100810    80         1  270  1.8  272.75   \n",
       "1          7149 2021-01-01 03:00:00  100920   110         3  300  1.7  271.25   \n",
       "2          7149 2021-01-01 06:00:00  100950    30         3  290  2.6  271.95   \n",
       "3          7149 2021-01-01 09:00:00  101100   150         2  280  1.7  272.45   \n",
       "4          7149 2021-01-01 12:00:00  101110    30         0   50  1.0  276.95   \n",
       "...         ...                 ...     ...   ...       ...  ...  ...     ...   \n",
       "3317       7149 2020-09-30 09:00:00  101540   -30         8  230  4.4  289.95   \n",
       "3318       7149 2020-09-30 12:00:00  101320  -210         8  190  4.9  292.05   \n",
       "3319       7149 2020-09-30 15:00:00  101140  -180         7  190  4.1  291.55   \n",
       "3320       7149 2020-09-30 18:00:00  101020  -130         6  190  2.7  290.15   \n",
       "3321       7149 2020-09-30 21:00:00  100880  -140         8  170  2.4  288.35   \n",
       "\n",
       "          td   u  ...  hnuage1  nnuage2  ctype2  hnuage2  nnuage3  ctype3  \\\n",
       "0     272.15  96  ...    600.0      NaN     NaN      NaN      NaN     NaN   \n",
       "1     270.95  98  ...   1500.0      2.0     3.0   3000.0      NaN     NaN   \n",
       "2     271.65  98  ...    480.0      4.0     6.0   2000.0      6.0     3.0   \n",
       "3     272.05  97  ...   1740.0      3.0     3.0   2800.0      NaN     NaN   \n",
       "4     274.15  82  ...    330.0      4.0     6.0    570.0      7.0     6.0   \n",
       "...      ...  ..  ...      ...      ...     ...      ...      ...     ...   \n",
       "3317  286.85  82  ...    400.0      7.0     6.0   2200.0      NaN     NaN   \n",
       "3318  285.55  66  ...    870.0      7.0     6.0   1900.0      NaN     NaN   \n",
       "3319  286.45  72  ...    820.0      7.0     6.0   2200.0      NaN     NaN   \n",
       "3320  285.25  73  ...   2160.0      NaN     NaN      NaN      NaN     NaN   \n",
       "3321  285.85  85  ...   1980.0      7.0     0.0   5700.0      NaN     NaN   \n",
       "\n",
       "      hnuage3  nnuage4  ctype4  hnuage4  \n",
       "0         NaN      NaN     NaN      NaN  \n",
       "1         NaN      NaN     NaN      NaN  \n",
       "2      3000.0      NaN     NaN      NaN  \n",
       "3         NaN      NaN     NaN      NaN  \n",
       "4       810.0      NaN     NaN      NaN  \n",
       "...       ...      ...     ...      ...  \n",
       "3317      NaN      NaN     NaN      NaN  \n",
       "3318      NaN      NaN     NaN      NaN  \n",
       "3319      NaN      NaN     NaN      NaN  \n",
       "3320      NaN      NaN     NaN      NaN  \n",
       "3321      NaN      NaN     NaN      NaN  \n",
       "\n",
       "[3322 rows x 59 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "external_conditions = pd.read_csv('data/external_data.csv')\n",
    "external_conditions['date'] = pd.to_datetime(external_conditions['date'])\n",
    "external_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with more than 40% NaN values\n",
    "threshold = len(external_conditions) * 0.4\n",
    "external_conditions = external_conditions.dropna(thresh=threshold, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/95/xl50cbyd26j4ngz698y18j3m0000gn/T/ipykernel_73608/758736975.py:15: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  date_range = pd.date_range(start=external_conditions['date'].min(), end=external_conditions['date'].max(), freq='H')\n",
      "/var/folders/95/xl50cbyd26j4ngz698y18j3m0000gn/T/ipykernel_73608/758736975.py:34: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "  closest_value = non_nan_values.iloc[(non_nan_values - value).abs().argmin()]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Sort the `external_conditions` DataFrame by the `date` column\n",
    "external_conditions = external_conditions.sort_values(by='date')\n",
    "\n",
    "# Drop columns with more than 40% NaN values\n",
    "threshold = len(external_conditions) * 0.4\n",
    "external_conditions = external_conditions.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "# Step 2: Remove duplicate entries based on the `date` column\n",
    "external_conditions = external_conditions.drop_duplicates(subset='date')\n",
    "\n",
    "# Step 3: Convert the 'date' column to datetime\n",
    "external_conditions['date'] = pd.to_datetime(external_conditions['date'])\n",
    "\n",
    "# Step 4: Create a complete date range from the minimum to the maximum date in the DataFrame\n",
    "date_range = pd.date_range(start=external_conditions['date'].min(), end=external_conditions['date'].max(), freq='H')\n",
    "\n",
    "# Step 5: Create a DataFrame from the date_range\n",
    "date_range_df = pd.DataFrame(date_range, columns=['date'])\n",
    "\n",
    "# Step 6: Merge the date_range DataFrame with the external_conditions DataFrame on the 'date' column\n",
    "full_external_conditions = pd.merge(date_range_df, external_conditions, on='date', how='left')\n",
    "\n",
    "# Fonction qui fait ce qu'on voulait faire avec ffill et bfill mais a la place prends la valeur la plus proche\n",
    "def fill_closest_value_all_columns(df):\n",
    "    \"\"\"Fill NaN values with the closest value for all numeric columns in the DataFrame.\"\"\"\n",
    "    filled_df = df.copy()\n",
    "    \n",
    "    for column in filled_df.columns:\n",
    "        if filled_df[column].dtype.kind in 'biufc':  # Numeric columns\n",
    "            non_nan_values = filled_df[column].dropna()\n",
    "            \n",
    "            def find_closest(value):\n",
    "                if pd.isna(value):\n",
    "                    closest_value = non_nan_values.iloc[(non_nan_values - value).abs().argmin()]\n",
    "                    return closest_value\n",
    "                return value\n",
    "            \n",
    "            filled_df[column] = filled_df[column].apply(find_closest)\n",
    "    \n",
    "    return filled_df\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "filled_external_conditions = fill_closest_value_all_columns(full_external_conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'date' column in external_conditions is of datetime type\n",
    "# filled_external_conditions['date'] = pd.to_datetime(filled_external_conditions['date'])\n",
    "\n",
    "# Merge the DataFrames\n",
    "merged_conditions = pd.merge(data, filled_external_conditions, on='date', how='left')\n",
    "\n",
    "merged_conditions = utils._column_rename(merged_conditions)\n",
    "\n",
    "\n",
    "merged_conditions_test = pd.merge(data_test, filled_external_conditions, on='date', how='left')\n",
    "\n",
    "merged_conditions_test = utils._column_rename(merged_conditions_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure \"date\" is in datetime format\n",
    "merged_conditions[\"date\"] = pd.to_datetime(merged_conditions[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with invalid datetime entries\n",
    "df = merged_conditions.dropna(subset=[\"date\"])\n",
    "\n",
    "# Extract date and time features\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "df[\"weekday\"] = df[\"date\"].dt.dayofweek\n",
    "df[\"day\"] = df[\"date\"].dt.day\n",
    "df[\"hour\"] = df[\"date\"].dt.hour\n",
    "df[\"is_weekend\"] = (df[\"weekday\"] >= 5).astype(int)\n",
    "\n",
    "# Handle school and public holidays\n",
    "unique_dates = df[\"date\"].dt.date.unique()\n",
    "d = SchoolHolidayDates()\n",
    "f = JoursFeries()\n",
    "\n",
    "try:\n",
    "    dict_school_holidays = {date: d.is_holiday_for_zone(date, \"C\") for date in unique_dates}\n",
    "    df[\"is_school_holiday\"] = df[\"date\"].dt.date.map(dict_school_holidays).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with school holidays mapping: {e}\")\n",
    "    df[\"is_school_holiday\"] = 0\n",
    "\n",
    "try:\n",
    "    dict_public_holidays = {date: f.is_bank_holiday(date, zone=\"Métropole\") for date in unique_dates}\n",
    "    df[\"is_public_holiday\"] = df[\"date\"].dt.date.map(dict_public_holidays).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with public holidays mapping: {e}\")\n",
    "    df[\"is_public_holiday\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure \"date\" is in datetime format\n",
    "merged_conditions_test[\"date\"] = pd.to_datetime(merged_conditions_test[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with invalid datetime entries\n",
    "df_test = merged_conditions_test.dropna(subset=[\"date\"])\n",
    "\n",
    "# Extract date and time features\n",
    "df_test[\"year\"] = df_test[\"date\"].dt.year\n",
    "df_test[\"month\"] = df_test[\"date\"].dt.month\n",
    "df_test[\"weekday\"] = df_test[\"date\"].dt.dayofweek\n",
    "df_test[\"day\"] = df_test[\"date\"].dt.day\n",
    "df_test[\"hour\"] = df_test[\"date\"].dt.hour\n",
    "df_test[\"is_weekend\"] = (df_test[\"weekday\"] >= 5).astype(int)\n",
    "\n",
    "# Handle school and public holidays\n",
    "unique_dates = df_test[\"date\"].dt.date.unique()\n",
    "d = SchoolHolidayDates()\n",
    "f = JoursFeries()\n",
    "\n",
    "try:\n",
    "    dict_school_holidays = {date: d.is_holiday_for_zone(date, \"C\") for date in unique_dates}\n",
    "    df_test[\"is_school_holiday\"] = df_test[\"date\"].dt.date.map(dict_school_holidays).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with school holidays mapping: {e}\")\n",
    "    df_test[\"is_school_holiday\"] = 0\n",
    "\n",
    "try:\n",
    "    dict_public_holidays = {date: f.is_bank_holiday(date, zone=\"Métropole\") for date in unique_dates}\n",
    "    df_test[\"is_public_holiday\"] = df_test[\"date\"].dt.date.map(dict_public_holidays).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with public holidays mapping: {e}\")\n",
    "    df_test[\"is_public_holiday\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TableReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TableReport(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test using flaml and the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from flaml import AutoML\n",
    "# from skrub import TableVectorizer\n",
    "# import logging\n",
    "\n",
    "# # Set up logging\n",
    "# logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "# logger = logging.getLogger()\n",
    "\n",
    "# # Preprocess the dataset\n",
    "# X = df.drop(columns=['log_bike_count', 'bike_count'])\n",
    "# y = df['log_bike_count']\n",
    "\n",
    "# # Split the data into training and validation sets based on the last 10% of dates\n",
    "# validation_split_index = int(len(df) * 0.9)\n",
    "# X_train, X_val = X.iloc[:validation_split_index], X.iloc[validation_split_index:]\n",
    "# y_train, y_val = y.iloc[:validation_split_index], y.iloc[validation_split_index:]\n",
    "\n",
    "# # Initialize the TableVectorizer\n",
    "# vectorizer = TableVectorizer()\n",
    "\n",
    "# # Fit and transform the training data\n",
    "# logger.info(\"Starting data transformation with TableVectorizer.\")\n",
    "# X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "# X_val_transformed = vectorizer.transform(X_val)\n",
    "# logger.info(\"Data transformation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize AutoML\n",
    "# automl = AutoML()\n",
    "\n",
    "# # FLAML settings with GPU support\n",
    "# automl_settings = {\n",
    "#     \"time_budget\": 300,  # Maximum time (in seconds)\n",
    "#     \"task\": \"regression\",  # Regression task\n",
    "#     \"metric\": \"r2\",  # Metric to optimize\n",
    "#     \"estimator_list\": [\"xgboost\", \"lgbm\", \"catboost\"],  # Algorithms compatible with GPU\n",
    "#     \"log_file_name\": \"flaml_gpu.log\",  # Logging\n",
    "#     \"seed\": 42,  # Reproducibility\n",
    "#     \"custom_hp\": {  # Specify static GPU configurations\n",
    "#         \"xgboost\": {\n",
    "#             \"tree_method\": \"gpu_hist\",  # Use GPU-accelerated histogram for XGBoost\n",
    "#         },\n",
    "#         \"lgbm\": {\n",
    "#             \"device\": \"gpu\",  # Use GPU for LightGBM\n",
    "#         },\n",
    "#         \"catboost\": {\n",
    "#             \"task_type\": \"GPU\",  # Use GPU for CatBoost\n",
    "#         },\n",
    "#     },\n",
    "#     \"verbose\": 1,  # Print logs during training\n",
    "#     \"eval_method\": \"holdout\",  # Use validation data explicitly\n",
    "#     \"log_type\": \"all\",\n",
    "#     \"keep_search_state\": True,  # Retain search state for detailed logging\n",
    "#     \"fit_kwargs_by_estimator\": {  # Avoid passing unsupported arguments\n",
    "#         \"xgboost\": {},  # Pass no extra arguments to XGBoost\n",
    "#         \"lgbm\": {},     # Pass no extra arguments to LightGBM\n",
    "#         \"catboost\": {}, # Pass no extra arguments to CatBoost\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# # Train AutoML\n",
    "# logger.info(\"Starting AutoML training.\")\n",
    "# automl.fit(X_train=X_train_transformed, y_train=y_train, X_val=X_val_transformed, y_val=y_val, **automl_settings)\n",
    "# logger.info(\"AutoML training completed.\")\n",
    "\n",
    "# # Results\n",
    "# print(\"Best estimator:\", automl.best_estimator)\n",
    "# print(\"Best hyperparameters:\", automl.best_config)\n",
    "# print(\"Best validation R²:\", 1 - automl.best_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test using Optuna hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import TableVectorizer\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preprocess the dataset using TableVectorizer\n",
    "X = df.drop(columns=['log_bike_count', 'bike_count'])\n",
    "y = df['log_bike_count']\n",
    "\n",
    "# # Split the data into training and validation sets based on the last 10% of dates\n",
    "# validation_split_index = int(len(df) * 0.9)\n",
    "# X_train, X_val = X.iloc[:validation_split_index], X.iloc[validation_split_index:]\n",
    "# y_train, y_val = y.iloc[:validation_split_index], y.iloc[validation_split_index:]\n",
    "\n",
    "# Initialize the TableVectorizer\n",
    "# vectorizer = TableVectorizer()\n",
    "\n",
    "# # Fit and transform the training data\n",
    "# X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "# X_val_transformed = vectorizer.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the objective function for Optuna\n",
    "# def objective(trial):\n",
    "#     param = {\n",
    "#         'objective': 'reg:squarederror',\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.1, 0.2),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "#         'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n",
    "#         'random_state': 42,\n",
    "#         'tree_method': 'gpu_hist',  # Enable GPU support\n",
    "#         'predictor': 'gpu_predictor'\n",
    "#     }\n",
    "#     model = XGBRegressor(**param)\n",
    "#     model.fit(X_train_transformed, y_train)\n",
    "#     return model.score(X_val_transformed, y_val)  # Maximizing validation R² score\n",
    "\n",
    "# # Create a study and optimize the objective function\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=50)\n",
    "\n",
    "# # Get the best parameters\n",
    "# best_params = study.best_params\n",
    "\n",
    "# # Add GPU-specific parameter to the best parameters\n",
    "# best_params['tree_method'] = 'gpu_hist'  # Ensure GPU is used for the final model\n",
    "\n",
    "# # Train the final model with the best parameters\n",
    "# X_transformed = vectorizer.transform(X)  # Transform the entire dataset\n",
    "# final_model = XGBRegressor(**best_params)\n",
    "# final_model.fit(X_transformed, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: XGBoost is not compiled with GPU support. Falling back to CPU.\n",
      "Trained model parameters:\n",
      "{'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8189577147756041, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.11986932069472364, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 374, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.6854480891474511, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from xgboost import XGBRegressor\n",
    "from skrub import TableVectorizer\n",
    "\n",
    "# Load the best parameters from the pickle file\n",
    "best_params = joblib.load('xg_boost_best_params.pkl')\n",
    "\n",
    "# Check if GPU support is available\n",
    "try:\n",
    "    import xgboost\n",
    "    if 'gpu_hist' in best_params.get('tree_method', '') and not xgboost.Booster().attr('gpu_id'):\n",
    "        print(\"Warning: XGBoost is not compiled with GPU support. Falling back to CPU.\")\n",
    "        best_params.pop('tree_method', None)  # Remove GPU-specific parameters\n",
    "        best_params.pop('predictor', None)\n",
    "except Exception as e:\n",
    "    print(f\"Error while checking GPU support: {e}\")\n",
    "\n",
    "# Initialize the TableVectorizer\n",
    "vectorizer = TableVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_transformed = vectorizer.fit_transform(X)\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "final_model = XGBRegressor(**best_params)\n",
    "final_model.fit(X_transformed, y)\n",
    "\n",
    "# Print model parameters\n",
    "print(\"Trained model parameters:\")\n",
    "print(final_model.get_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0.8988337 0.5563414 1.1762435 ... 3.8181887 2.9900236 2.774225 ]\n"
     ]
    }
   ],
   "source": [
    "# Transform the test data using the same vectorizer instance\n",
    "X_test_transformed = vectorizer.transform(df_test)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = final_model.predict(X_test_transformed)\n",
    "\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(y_pred, columns=[\"log_bike_count\"])\n",
    "df_submission.index = data_test.index\n",
    "df_submission.index.name = \"Id\"\n",
    "df_submission.to_csv(\"/Users/felix/Documents/X/Cours Python/Kaggle/submission/test_pipeline.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_parquet('data/final_test.parquet')\n",
    "# Merge the DataFrames\n",
    "merged_conditions = pd.merge(test_data, filled_external_conditions, on='date', how='left')\n",
    "\n",
    "merged_conditions = utils._column_rename(merged_conditions)\n",
    "\n",
    "# Ensure \"date\" is in datetime format\n",
    "merged_conditions[\"date\"] = pd.to_datetime(merged_conditions[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with invalid datetime entries\n",
    "df_test = merged_conditions.dropna(subset=[\"date\"])\n",
    "\n",
    "# Extract date and time features\n",
    "df_test[\"year\"] = df_test[\"date\"].dt.year\n",
    "df_test[\"month\"] = df_test[\"date\"].dt.month\n",
    "df_test[\"weekday\"] = df_test[\"date\"].dt.dayofweek\n",
    "df_test[\"day\"] = df_test[\"date\"].dt.day\n",
    "df_test[\"hour\"] = df_test[\"date\"].dt.hour\n",
    "df_test[\"is_weekend\"] = (df_test[\"weekday\"] >= 5).astype(int)\n",
    "\n",
    "# Handle school and public holidays\n",
    "unique_dates_test = df_test[\"date\"].dt.date.unique()\n",
    "\n",
    "try:\n",
    "    dict_school_holidays_test = {date: d.is_holiday_for_zone(date, \"C\") for date in unique_dates_test}\n",
    "    df_test[\"is_school_holiday\"] = df_test[\"date\"].dt.date.map(dict_school_holidays_test).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with school holidays mapping: {e}\")\n",
    "    df_test[\"is_school_holiday\"] = 0\n",
    "\n",
    "try:\n",
    "    dict_public_holidays_test = {date: f.is_bank_holiday(date, zone=\"Métropole\") for date in unique_dates_test}\n",
    "    df_test[\"is_public_holiday\"] = df_test[\"date\"].dt.date.map(dict_public_holidays_test).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with public holidays mapping: {e}\")\n",
    "    df_test[\"is_public_holiday\"] = 0\n",
    "\n",
    "# Predict using the pipeline\n",
    "y_pred_test = pipeline.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(y_pred_test, columns=[\"log_bike_count\"])\n",
    "df_submission.index.name = \"Id\"\n",
    "df_submission\n",
    "df_submission.to_csv(\"/Users/felix/Documents/X/Cours Python/Kaggle/submission/test_pipeline.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En dessous c'est des tests d'avant ca ne fait pas tourner ce qui marche actuellement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new category to categorical columns\n",
    "for col in df.select_dtypes(include=['category']).columns:\n",
    "\tdf[col] = df[col].cat.add_categories([0])\n",
    "\n",
    "# Fill NaN values with 0\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df['log_bike_count'].values\n",
    "X_train = df.drop(['log_bike_count', \"bike_count\"], axis=1)\n",
    "\n",
    "date_cols = [\"year\", \"month\", \"weekday\", \"day\", \"hour\", \"is_weekend\", \"is_school_holiday\", \"is_public_holiday\"]\n",
    "categorical_cols = [\"counter_name\"]\n",
    "numerical_cols = [\n",
    "    'latitude', 'longitude', 'Sea Level Pressure (hPa)', 'Pressure Tendency (hPa/3h)',\n",
    "    'Pressure Tendency Code', 'Wind Direction (°)', 'Wind Speed (m/s)', 'Air Temperature (°C)',\n",
    "    'Dew Point Temperature (°C)', 'Relative Humidity (%)', 'Visibility (m)', 'Present Weather Code',\n",
    "    'Past Weather Code 1', 'Past Weather Code 2', 'Total Cloud Cover (oktas)', 'Cloud Base Height (m)',\n",
    "    'Lowest Cloud Base Height (m)', 'Low Cloud Type', 'Station Level Pressure (hPa)', '24h Pressure Tendency (hPa)',\n",
    "    '10min Max Wind Gust (m/s)', 'Max Wind Gust (m/s)', 'Measurement Period Duration', 'Ground State',\n",
    "    'Snow Height (cm)', 'New Snow Depth (cm)', 'New Snowfall Duration (hours)', 'Rainfall (1h, mm)',\n",
    "    'Rainfall (3h, mm)', 'Rainfall (6h, mm)', 'Rainfall (12h, mm)', 'Rainfall (24h, mm)',\n",
    "    'Layer 1 Cloud Cover (oktas)', 'Layer 1 Cloud Type', 'Layer 1 Cloud Base Height (m)'\n",
    "]\n",
    "\n",
    "\n",
    "# 1. Apply column transformations\n",
    "# One-hot encode date columns\n",
    "date_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "date_encoded = date_encoder.fit_transform(X_train[date_cols])\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "cat_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "cat_encoded = cat_encoder.fit_transform(X_train[categorical_cols])\n",
    "\n",
    "# Standard scale numerical columns\n",
    "num_scaler = StandardScaler()\n",
    "num_scaled = num_scaler.fit_transform(X_train[numerical_cols])\n",
    "\n",
    "X_transformed = hstack([date_encoded, cat_encoded, num_scaled]).toarray()\n",
    "\n",
    "# 2. Train the model\n",
    "model = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_transformed, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = utils.get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames\n",
    "merged_conditions = pd.merge(X_test, external_conditions, on='date', how='left')\n",
    "\n",
    "merged_conditions = utils._column_rename(merged_conditions)\n",
    "# Ensure \"date\" is in datetime format\n",
    "merged_conditions[\"date\"] = pd.to_datetime(merged_conditions[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with invalid datetime entries\n",
    "df = merged_conditions.dropna(subset=[\"date\"])\n",
    "\n",
    "# Extract date and time features\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "df[\"weekday\"] = df[\"date\"].dt.dayofweek\n",
    "df[\"day\"] = df[\"date\"].dt.day\n",
    "df[\"hour\"] = df[\"date\"].dt.hour\n",
    "df[\"is_weekend\"] = (df[\"weekday\"] >= 5).astype(int)\n",
    "\n",
    "# Handle school and public holidays\n",
    "unique_dates = df[\"date\"].dt.date.unique()\n",
    "d = SchoolHolidayDates()\n",
    "f = JoursFeries()\n",
    "\n",
    "try:\n",
    "    dict_school_holidays = {date: d.is_holiday_for_zone(date, \"C\") for date in unique_dates}\n",
    "    df[\"is_school_holiday\"] = df[\"date\"].dt.date.map(dict_school_holidays).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with school holidays mapping: {e}\")\n",
    "    df[\"is_school_holiday\"] = 0\n",
    "\n",
    "try:\n",
    "    dict_public_holidays = {date: f.is_bank_holiday(date, zone=\"Métropole\") for date in unique_dates}\n",
    "    df[\"is_public_holiday\"] = df[\"date\"].dt.date.map(dict_public_holidays).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with public holidays mapping: {e}\")\n",
    "    df[\"is_public_holiday\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the test data with the same transformations as the training data\n",
    "# 1. Apply column transformations\n",
    "# One-hot encode date columns\n",
    "date_encoded_test = date_encoder.transform(df[date_cols])\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "cat_encoded_test = cat_encoder.transform(df[categorical_cols])\n",
    "\n",
    "# Standard scale numerical columns\n",
    "num_scaled_test = num_scaler.transform(df[numerical_cols])\n",
    "\n",
    "# Combine all transformed features\n",
    "X_test_transformed_numeric = hstack([date_encoded_test, cat_encoded_test, num_scaled_test]).toarray()\n",
    "\n",
    "# 2. Make predictions\n",
    "y_pred = model.predict(X_test_transformed_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(y_pred, columns=[\"log_bike_count\"])\n",
    "df_submission.index.name = \"Id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv(\"/Users/felix/Documents/X/Cours Python/Kaggle/submission/test_pipeline.csv\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
