{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.sparse import hstack\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skrub import TableReport\n",
    "from jours_feries_france import JoursFeries\n",
    "from vacances_scolaires_france import SchoolHolidayDates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"data/train.parquet\")\n",
    "# Sort by date first, so that time based cross-validation would produce correct results\n",
    "data = data.sort_values([\"date\", \"counter_name\"])\n",
    "\n",
    "data_test = pd.read_parquet(\"data/final_test.parquet\")\n",
    "# Sort by date first, so that time based cross-validation would produce correct results\n",
    "data_test = data_test.sort_values([\"date\", \"counter_name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_conditions = pd.read_csv('data/external_data.csv')\n",
    "external_conditions['date'] = pd.to_datetime(external_conditions['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with more than 40% NaN values\n",
    "threshold = len(external_conditions) * 0.4\n",
    "external_conditions = external_conditions.dropna(thresh=threshold, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21689/758736975.py:15: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  date_range = pd.date_range(start=external_conditions['date'].min(), end=external_conditions['date'].max(), freq='H')\n",
      "/tmp/ipykernel_21689/758736975.py:34: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "  closest_value = non_nan_values.iloc[(non_nan_values - value).abs().argmin()]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Sort the `external_conditions` DataFrame by the `date` column\n",
    "external_conditions = external_conditions.sort_values(by='date')\n",
    "\n",
    "# Drop columns with more than 40% NaN values\n",
    "threshold = len(external_conditions) * 0.4\n",
    "external_conditions = external_conditions.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "# Step 2: Remove duplicate entries based on the `date` column\n",
    "external_conditions = external_conditions.drop_duplicates(subset='date')\n",
    "\n",
    "# Step 3: Convert the 'date' column to datetime\n",
    "external_conditions['date'] = pd.to_datetime(external_conditions['date'])\n",
    "\n",
    "# Step 4: Create a complete date range from the minimum to the maximum date in the DataFrame\n",
    "date_range = pd.date_range(start=external_conditions['date'].min(), end=external_conditions['date'].max(), freq='h')\n",
    "\n",
    "# Step 5: Create a DataFrame from the date_range\n",
    "date_range_df = pd.DataFrame(date_range, columns=['date'])\n",
    "\n",
    "# Step 6: Merge the date_range DataFrame with the external_conditions DataFrame on the 'date' column\n",
    "full_external_conditions = pd.merge(date_range_df, external_conditions, on='date', how='left')\n",
    "\n",
    "# Fonction qui fait ce qu'on voulait faire avec ffill et bfill mais a la place prends la valeur la plus proche\n",
    "def fill_closest_value_all_columns(df):\n",
    "    \"\"\"Fill NaN values with the closest value for all numeric columns in the DataFrame.\"\"\"\n",
    "    filled_df = df.copy()\n",
    "    \n",
    "    for column in filled_df.columns:\n",
    "        if filled_df[column].dtype.kind in 'biufc':  # Numeric columns\n",
    "            non_nan_values = filled_df[column].dropna()\n",
    "            \n",
    "            def find_closest(value):\n",
    "                if pd.isna(value):\n",
    "                    closest_value = non_nan_values.iloc[(non_nan_values - value).abs().argmin()]\n",
    "                    return closest_value\n",
    "                return value\n",
    "            \n",
    "            filled_df[column] = filled_df[column].apply(find_closest)\n",
    "    \n",
    "    return filled_df\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "filled_external_conditions = fill_closest_value_all_columns(full_external_conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'date' column in external_conditions is of datetime type\n",
    "# filled_external_conditions['date'] = pd.to_datetime(filled_external_conditions['date'])\n",
    "\n",
    "# Merge the DataFrames\n",
    "merged_conditions = pd.merge(data, filled_external_conditions, on='date', how='left')\n",
    "\n",
    "merged_conditions = utils._column_rename(merged_conditions)\n",
    "\n",
    "\n",
    "merged_conditions_test = pd.merge(data_test, filled_external_conditions, on='date', how='left')\n",
    "\n",
    "merged_conditions_test = utils._column_rename(merged_conditions_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure \"date\" is in datetime format\n",
    "merged_conditions[\"date\"] = pd.to_datetime(merged_conditions[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with invalid datetime entries\n",
    "df = merged_conditions.dropna(subset=[\"date\"])\n",
    "\n",
    "# Extract date and time features\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "df[\"weekday\"] = df[\"date\"].dt.dayofweek\n",
    "df[\"day\"] = df[\"date\"].dt.day\n",
    "df[\"hour\"] = df[\"date\"].dt.hour\n",
    "df[\"is_weekend\"] = (df[\"weekday\"] >= 5).astype(int)\n",
    "\n",
    "# Handle school and public holidays\n",
    "unique_dates = df[\"date\"].dt.date.unique()\n",
    "d = SchoolHolidayDates()\n",
    "f = JoursFeries()\n",
    "\n",
    "try:\n",
    "    dict_school_holidays = {date: d.is_holiday_for_zone(date, \"C\") for date in unique_dates}\n",
    "    df[\"is_school_holiday\"] = df[\"date\"].dt.date.map(dict_school_holidays).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with school holidays mapping: {e}\")\n",
    "    df[\"is_school_holiday\"] = 0\n",
    "\n",
    "try:\n",
    "    dict_public_holidays = {date: f.is_bank_holiday(date, zone=\"Métropole\") for date in unique_dates}\n",
    "    df[\"is_public_holiday\"] = df[\"date\"].dt.date.map(dict_public_holidays).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with public holidays mapping: {e}\")\n",
    "    df[\"is_public_holiday\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure \"date\" is in datetime format\n",
    "merged_conditions_test[\"date\"] = pd.to_datetime(merged_conditions_test[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with invalid datetime entries\n",
    "df_test = merged_conditions_test.dropna(subset=[\"date\"])\n",
    "\n",
    "# Extract date and time features\n",
    "df_test[\"year\"] = df_test[\"date\"].dt.year\n",
    "df_test[\"month\"] = df_test[\"date\"].dt.month\n",
    "df_test[\"weekday\"] = df_test[\"date\"].dt.dayofweek\n",
    "df_test[\"day\"] = df_test[\"date\"].dt.day\n",
    "df_test[\"hour\"] = df_test[\"date\"].dt.hour\n",
    "df_test[\"is_weekend\"] = (df_test[\"weekday\"] >= 5).astype(int)\n",
    "\n",
    "# Handle school and public holidays\n",
    "unique_dates = df_test[\"date\"].dt.date.unique()\n",
    "d = SchoolHolidayDates()\n",
    "f = JoursFeries()\n",
    "\n",
    "try:\n",
    "    dict_school_holidays = {date: d.is_holiday_for_zone(date, \"C\") for date in unique_dates}\n",
    "    df_test[\"is_school_holiday\"] = df_test[\"date\"].dt.date.map(dict_school_holidays).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with school holidays mapping: {e}\")\n",
    "    df_test[\"is_school_holiday\"] = 0\n",
    "\n",
    "try:\n",
    "    dict_public_holidays = {date: f.is_bank_holiday(date, zone=\"Métropole\") for date in unique_dates}\n",
    "    df_test[\"is_public_holiday\"] = df_test[\"date\"].dt.date.map(dict_public_holidays).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with public holidays mapping: {e}\")\n",
    "    df_test[\"is_public_holiday\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TableReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TableReport(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test using flaml and the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from flaml import AutoML\n",
    "# from skrub import TableVectorizer\n",
    "# import logging\n",
    "\n",
    "# # Set up logging\n",
    "# logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "# logger = logging.getLogger()\n",
    "\n",
    "# # Preprocess the dataset\n",
    "# X = df.drop(columns=['log_bike_count', 'bike_count'])\n",
    "# y = df['log_bike_count']\n",
    "\n",
    "# # Split the data into training and validation sets based on the last 10% of dates\n",
    "# validation_split_index = int(len(df) * 0.9)\n",
    "# X_train, X_val = X.iloc[:validation_split_index], X.iloc[validation_split_index:]\n",
    "# y_train, y_val = y.iloc[:validation_split_index], y.iloc[validation_split_index:]\n",
    "\n",
    "# # Initialize the TableVectorizer\n",
    "# vectorizer = TableVectorizer()\n",
    "\n",
    "# # Fit and transform the training data\n",
    "# logger.info(\"Starting data transformation with TableVectorizer.\")\n",
    "# X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "# X_val_transformed = vectorizer.transform(X_val)\n",
    "# logger.info(\"Data transformation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize AutoML\n",
    "# automl = AutoML()\n",
    "\n",
    "# # FLAML settings with GPU support\n",
    "# automl_settings = {\n",
    "#     \"time_budget\": 300,  # Maximum time (in seconds)\n",
    "#     \"task\": \"regression\",  # Regression task\n",
    "#     \"metric\": \"r2\",  # Metric to optimize\n",
    "#     \"estimator_list\": [\"xgboost\", \"lgbm\", \"catboost\"],  # Algorithms compatible with GPU\n",
    "#     \"log_file_name\": \"flaml_gpu.log\",  # Logging\n",
    "#     \"seed\": 42,  # Reproducibility\n",
    "#     \"custom_hp\": {  # Specify static GPU configurations\n",
    "#         \"xgboost\": {\n",
    "#             \"tree_method\": \"gpu_hist\",  # Use GPU-accelerated histogram for XGBoost\n",
    "#         },\n",
    "#         \"lgbm\": {\n",
    "#             \"device\": \"gpu\",  # Use GPU for LightGBM\n",
    "#         },\n",
    "#         \"catboost\": {\n",
    "#             \"task_type\": \"GPU\",  # Use GPU for CatBoost\n",
    "#         },\n",
    "#     },\n",
    "#     \"verbose\": 1,  # Print logs during training\n",
    "#     \"eval_method\": \"holdout\",  # Use validation data explicitly\n",
    "#     \"log_type\": \"all\",\n",
    "#     \"keep_search_state\": True,  # Retain search state for detailed logging\n",
    "#     \"fit_kwargs_by_estimator\": {  # Avoid passing unsupported arguments\n",
    "#         \"xgboost\": {},  # Pass no extra arguments to XGBoost\n",
    "#         \"lgbm\": {},     # Pass no extra arguments to LightGBM\n",
    "#         \"catboost\": {}, # Pass no extra arguments to CatBoost\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# # Train AutoML\n",
    "# logger.info(\"Starting AutoML training.\")\n",
    "# automl.fit(X_train=X_train_transformed, y_train=y_train, X_val=X_val_transformed, y_val=y_val, **automl_settings)\n",
    "# logger.info(\"AutoML training completed.\")\n",
    "\n",
    "# # Results\n",
    "# print(\"Best estimator:\", automl.best_estimator)\n",
    "# print(\"Best hyperparameters:\", automl.best_config)\n",
    "# print(\"Best validation R²:\", 1 - automl.best_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test using Optuna hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import TableVectorizer\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preprocess the dataset using TableVectorizer\n",
    "X = df.drop(columns=['log_bike_count', 'bike_count'])\n",
    "y = df['log_bike_count']\n",
    "\n",
    "# # Split the data into training and validation sets based on the last 10% of dates\n",
    "# validation_split_index = int(len(df) * 0.9)\n",
    "# X_train, X_val = X.iloc[:validation_split_index], X.iloc[validation_split_index:]\n",
    "# y_train, y_val = y.iloc[:validation_split_index], y.iloc[validation_split_index:]\n",
    "\n",
    "# Initialize the TableVectorizer\n",
    "# vectorizer = TableVectorizer()\n",
    "\n",
    "# # Fit and transform the training data\n",
    "# X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "# X_val_transformed = vectorizer.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the objective function for Optuna\n",
    "# def objective(trial):\n",
    "#     param = {\n",
    "#         'objective': 'reg:squarederror',\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.1, 0.2),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "#         'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n",
    "#         'random_state': 42,\n",
    "#         'tree_method': 'gpu_hist',  # Enable GPU support\n",
    "#         'predictor': 'gpu_predictor'\n",
    "#     }\n",
    "#     model = XGBRegressor(**param)\n",
    "#     model.fit(X_train_transformed, y_train)\n",
    "#     return model.score(X_val_transformed, y_val)  # Maximizing validation R² score\n",
    "\n",
    "# # Create a study and optimize the objective function\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=50)\n",
    "\n",
    "# # Get the best parameters\n",
    "# best_params = study.best_params\n",
    "\n",
    "# # Add GPU-specific parameter to the best parameters\n",
    "# best_params['tree_method'] = 'gpu_hist'  # Ensure GPU is used for the final model\n",
    "\n",
    "# # Train the final model with the best parameters\n",
    "# X_transformed = vectorizer.transform(X)  # Transform the entire dataset\n",
    "# final_model = XGBRegressor(**best_params)\n",
    "# final_model.fit(X_transformed, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: XGBoost is not compiled with GPU support. Falling back to CPU.\n",
      "Trained model parameters:\n",
      "{'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8189577147756041, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.11986932069472364, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 374, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.6854480891474511, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from xgboost import XGBRegressor\n",
    "from skrub import TableVectorizer\n",
    "\n",
    "# Load the best parameters from the pickle file\n",
    "best_params = joblib.load('xg_boost_best_params.pkl')\n",
    "\n",
    "# Check if GPU support is available\n",
    "try:\n",
    "    import xgboost\n",
    "    if 'gpu_hist' in best_params.get('tree_method', '') and not xgboost.Booster().attr('gpu_id'):\n",
    "        print(\"Warning: XGBoost is not compiled with GPU support. Falling back to CPU.\")\n",
    "        best_params.pop('tree_method', None)  # Remove GPU-specific parameters\n",
    "        best_params.pop('predictor', None)\n",
    "except Exception as e:\n",
    "    print(f\"Error while checking GPU support: {e}\")\n",
    "\n",
    "# Initialize the TableVectorizer\n",
    "vectorizer = TableVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_transformed = vectorizer.fit_transform(X)\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "final_model = XGBRegressor(**best_params)\n",
    "final_model.fit(X_transformed, y)\n",
    "\n",
    "# Print model parameters\n",
    "print(\"Trained model parameters:\")\n",
    "print(final_model.get_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0.8988337 0.5563414 1.1762435 ... 3.8181887 2.9900236 2.774225 ]\n"
     ]
    }
   ],
   "source": [
    "# Transform the test data using the same vectorizer instance\n",
    "X_test_transformed = vectorizer.transform(df_test)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = final_model.predict(X_test_transformed)\n",
    "\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(y_pred, columns=[\"log_bike_count\"])\n",
    "df_submission.index = data_test.index\n",
    "df_submission.index.name = \"Id\"\n",
    "df_submission.to_csv(\"/Users/felix/Documents/X/Cours Python/Kaggle/submission/test_pipeline.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_parquet('data/final_test.parquet')\n",
    "# Merge the DataFrames\n",
    "merged_conditions = pd.merge(test_data, filled_external_conditions, on='date', how='left')\n",
    "\n",
    "merged_conditions = utils._column_rename(merged_conditions)\n",
    "\n",
    "# Ensure \"date\" is in datetime format\n",
    "merged_conditions[\"date\"] = pd.to_datetime(merged_conditions[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with invalid datetime entries\n",
    "df_test = merged_conditions.dropna(subset=[\"date\"])\n",
    "\n",
    "# Extract date and time features\n",
    "df_test[\"year\"] = df_test[\"date\"].dt.year\n",
    "df_test[\"month\"] = df_test[\"date\"].dt.month\n",
    "df_test[\"weekday\"] = df_test[\"date\"].dt.dayofweek\n",
    "df_test[\"day\"] = df_test[\"date\"].dt.day\n",
    "df_test[\"hour\"] = df_test[\"date\"].dt.hour\n",
    "df_test[\"is_weekend\"] = (df_test[\"weekday\"] >= 5).astype(int)\n",
    "\n",
    "# Handle school and public holidays\n",
    "unique_dates_test = df_test[\"date\"].dt.date.unique()\n",
    "\n",
    "try:\n",
    "    dict_school_holidays_test = {date: d.is_holiday_for_zone(date, \"C\") for date in unique_dates_test}\n",
    "    df_test[\"is_school_holiday\"] = df_test[\"date\"].dt.date.map(dict_school_holidays_test).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with school holidays mapping: {e}\")\n",
    "    df_test[\"is_school_holiday\"] = 0\n",
    "\n",
    "try:\n",
    "    dict_public_holidays_test = {date: f.is_bank_holiday(date, zone=\"Métropole\") for date in unique_dates_test}\n",
    "    df_test[\"is_public_holiday\"] = df_test[\"date\"].dt.date.map(dict_public_holidays_test).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with public holidays mapping: {e}\")\n",
    "    df_test[\"is_public_holiday\"] = 0\n",
    "\n",
    "# Predict using the pipeline\n",
    "y_pred_test = pipeline.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(y_pred_test, columns=[\"log_bike_count\"])\n",
    "df_submission.index.name = \"Id\"\n",
    "df_submission\n",
    "df_submission.to_csv(\"/Users/felix/Documents/X/Cours Python/Kaggle/submission/test_pipeline.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En dessous c'est des tests d'avant ca ne fait pas tourner ce qui marche actuellement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new category to categorical columns\n",
    "for col in df.select_dtypes(include=['category']).columns:\n",
    "\tdf[col] = df[col].cat.add_categories([0])\n",
    "\n",
    "# Fill NaN values with 0\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df['log_bike_count'].values\n",
    "X_train = df.drop(['log_bike_count', \"bike_count\"], axis=1)\n",
    "\n",
    "date_cols = [\"year\", \"month\", \"weekday\", \"day\", \"hour\", \"is_weekend\", \"is_school_holiday\", \"is_public_holiday\"]\n",
    "categorical_cols = [\"counter_name\"]\n",
    "numerical_cols = [\n",
    "    'latitude', 'longitude', 'Sea Level Pressure (hPa)', 'Pressure Tendency (hPa/3h)',\n",
    "    'Pressure Tendency Code', 'Wind Direction (°)', 'Wind Speed (m/s)', 'Air Temperature (°C)',\n",
    "    'Dew Point Temperature (°C)', 'Relative Humidity (%)', 'Visibility (m)', 'Present Weather Code',\n",
    "    'Past Weather Code 1', 'Past Weather Code 2', 'Total Cloud Cover (oktas)', 'Cloud Base Height (m)',\n",
    "    'Lowest Cloud Base Height (m)', 'Low Cloud Type', 'Station Level Pressure (hPa)', '24h Pressure Tendency (hPa)',\n",
    "    '10min Max Wind Gust (m/s)', 'Max Wind Gust (m/s)', 'Measurement Period Duration', 'Ground State',\n",
    "    'Snow Height (cm)', 'New Snow Depth (cm)', 'New Snowfall Duration (hours)', 'Rainfall (1h, mm)',\n",
    "    'Rainfall (3h, mm)', 'Rainfall (6h, mm)', 'Rainfall (12h, mm)', 'Rainfall (24h, mm)',\n",
    "    'Layer 1 Cloud Cover (oktas)', 'Layer 1 Cloud Type', 'Layer 1 Cloud Base Height (m)'\n",
    "]\n",
    "\n",
    "\n",
    "# 1. Apply column transformations\n",
    "# One-hot encode date columns\n",
    "date_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "date_encoded = date_encoder.fit_transform(X_train[date_cols])\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "cat_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "cat_encoded = cat_encoder.fit_transform(X_train[categorical_cols])\n",
    "\n",
    "# Standard scale numerical columns\n",
    "num_scaler = StandardScaler()\n",
    "num_scaled = num_scaler.fit_transform(X_train[numerical_cols])\n",
    "\n",
    "X_transformed = hstack([date_encoded, cat_encoded, num_scaled]).toarray()\n",
    "\n",
    "# 2. Train the model\n",
    "model = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_transformed, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = utils.get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames\n",
    "merged_conditions = pd.merge(X_test, external_conditions, on='date', how='left')\n",
    "\n",
    "merged_conditions = utils._column_rename(merged_conditions)\n",
    "# Ensure \"date\" is in datetime format\n",
    "merged_conditions[\"date\"] = pd.to_datetime(merged_conditions[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with invalid datetime entries\n",
    "df = merged_conditions.dropna(subset=[\"date\"])\n",
    "\n",
    "# Extract date and time features\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "df[\"weekday\"] = df[\"date\"].dt.dayofweek\n",
    "df[\"day\"] = df[\"date\"].dt.day\n",
    "df[\"hour\"] = df[\"date\"].dt.hour\n",
    "df[\"is_weekend\"] = (df[\"weekday\"] >= 5).astype(int)\n",
    "\n",
    "# Handle school and public holidays\n",
    "unique_dates = df[\"date\"].dt.date.unique()\n",
    "d = SchoolHolidayDates()\n",
    "f = JoursFeries()\n",
    "\n",
    "try:\n",
    "    dict_school_holidays = {date: d.is_holiday_for_zone(date, \"C\") for date in unique_dates}\n",
    "    df[\"is_school_holiday\"] = df[\"date\"].dt.date.map(dict_school_holidays).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with school holidays mapping: {e}\")\n",
    "    df[\"is_school_holiday\"] = 0\n",
    "\n",
    "try:\n",
    "    dict_public_holidays = {date: f.is_bank_holiday(date, zone=\"Métropole\") for date in unique_dates}\n",
    "    df[\"is_public_holiday\"] = df[\"date\"].dt.date.map(dict_public_holidays).fillna(0).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error with public holidays mapping: {e}\")\n",
    "    df[\"is_public_holiday\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the test data with the same transformations as the training data\n",
    "# 1. Apply column transformations\n",
    "# One-hot encode date columns\n",
    "date_encoded_test = date_encoder.transform(df[date_cols])\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "cat_encoded_test = cat_encoder.transform(df[categorical_cols])\n",
    "\n",
    "# Standard scale numerical columns\n",
    "num_scaled_test = num_scaler.transform(df[numerical_cols])\n",
    "\n",
    "# Combine all transformed features\n",
    "X_test_transformed_numeric = hstack([date_encoded_test, cat_encoded_test, num_scaled_test]).toarray()\n",
    "\n",
    "# 2. Make predictions\n",
    "y_pred = model.predict(X_test_transformed_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(y_pred, columns=[\"log_bike_count\"])\n",
    "df_submission.index.name = \"Id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv(\"/Users/felix/Documents/X/Cours Python/Kaggle/submission/test_pipeline.csv\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
